from django.shortcuts import render, redirect
from django.http import JsonResponse, StreamingHttpResponse, HttpResponse
from django.conf import settings
from django.template.loader import render_to_string
from django.views.decorators.http import require_POST
from django.views.decorators.csrf import csrf_exempt
from asgiref.sync import async_to_sync, sync_to_async
import os, json, threading, time
import google.generativeai as genai
from dotenv import load_dotenv

from .search_manager import get_search, delete_search as delete_search_manager, load_results
from .export_utils import export_all, prune_old_exports, audit_exports

# Cargar variables de entorno desde .env
load_dotenv()

# Sistema de control de b√∫squedas activas
active_searches = {}
search_lock = threading.Lock()

# Configurar API key s√≥lo si existe
API_KEY = os.environ.get("GEMINI_API_KEY")
if API_KEY:
    try:
        genai.configure(api_key=API_KEY)
        print("[DEPURACI√ìN] GEMINI_API_KEY configurada correctamente desde .env")
    except Exception as e:
        print(f"[DEPURACI√ìN] No se pudo configurar Gemini: {e}")
else:
    print("[DEPURACI√ìN] GEMINI_API_KEY no configurada. Se usar√° modo fallback.")

def home(request):
    # Cargar b√∫squedas guardadas para mostrar en la p√°gina
    from .search_manager import get_all_searches
    searches = get_all_searches()
    print(f"[DEPURACI√ìN] B√∫squedas guardadas encontradas: {len(searches)}")
    return render(request, 'core/home.html', {'searches': searches})

def search_detail(request, search_id):
    print(f"[DEPURACI√ìN] Iniciando b√∫squeda para search_id={search_id}")
    search = get_search(search_id)
    print(f"[DEPURACI√ìN] Datos de b√∫squeda: {search}")
    results = load_results(search_id)
    print(f"[DEPURACI√ìN] Resultados cargados: {len(results) if results else 0}")
    advertencias = ["El texto de la b√∫squeda encuentra las palabras por separado en las publicaciones."]
    print(f"[DEPURACI√ìN] Renderizando resultados...")
    return render(request, 'core/search_detail_partial.html', {'search': search, 'results': results, 'advertencias': advertencias})

@require_POST
def ia_sugerir_filtros(request):
    """Endpoint s√≠ncrono que llama a la funci√≥n async de an√°lisis v√≠a async_to_sync."""
    try:
        body = json.loads(request.body.decode('utf-8') or '{}')
        texto = body.get('texto', '')
        filtros_manual = body.get('filtros', {}) or {}
        print(f"[DEPURACI√ìN][IA SUGERIR] Texto recibido: {texto}")
        print(f"[DEPURACI√ìN][IA SUGERIR] Filtros manuales: {filtros_manual}")
        ia_result = async_to_sync(analyze_query_with_ia)(texto)
        ia_filters = ia_result.get('filters', {})
        # Prioriza filtros provenientes del texto (IA)
        filtros_final = {**filtros_manual, **ia_filters}
        print(f"[DEPURACI√ìN][IA SUGERIR] Filtros IA: {ia_filters}")
        print(f"[DEPURACI√ìN][IA SUGERIR] Filtros fusionados: {filtros_final}")
        import datetime
        marca_tiempo = datetime.datetime.now().isoformat()
        return JsonResponse({
            'filters': filtros_final,
            'keywords': ia_result.get('keywords', []),
            'remaining_text': ia_result.get('remaining_text', ''),
            'original_text': texto,
            'datetime': marca_tiempo
        })
    except Exception as e:
        print(f"[DEPURACI√ìN] Error en ia_sugerir_filtros: {e}")
        return JsonResponse({'error': str(e)}, status=500)

async def analyze_query_with_ia(query: str) -> dict:
    """Analiza texto libre y retorna dict con filters y remaining_text.
    Fallback: si no hay API key o error, devuelve estructura vac√≠a.
    """
    if not query:
        return {"filters": {}, "remaining_text": ""}

    prompt = (
        "Eres un asistente para b√∫squeda inmobiliaria en Uruguay. Tu tarea es extraer filtros estructurados del texto y tambi√©n identificar palabras clave relevantes para la b√∫squeda. "
        "Devuelve SOLO un JSON con las siguientes keys: "
        "- filters: solo los filtros soportados (ver lista abajo), NO inventes filtros nuevos. "
        "- keywords: palabras o frases relevantes para la b√∫squeda que no coincidan con los filtros. Si el usuario menciona condiciones, restricciones o detalles para los que NO hay filtro estructurado (por ejemplo, gastos comunes, distancia a la rambla, barrio espec√≠fico, cercan√≠a a lugares, etc.), agrega la frase completa como keyword. "
        "- remaining_text: el texto restante que no fue usado para filtros ni keywords. "
        "Campos soportados como filtros: "
        "- departamento: Montevideo, Canelones, Maldonado, Rocha, Colonia, San Jos√©, Florida, Lavalleja, Rivera, Tacuaremb√≥, Salto, Paysand√∫, Artigas, Durazno, Treinta y Tres, Cerro Largo, R√≠o Negro, Flores, Soriano. "
        "- ciudad: Aguada, Pocitos, Carrasco, Centro, Cord√≥n, Malv√≠n, Buceo, Parque Batlle, Punta Carretas, La Blanqueada, Tres Cruces, Sayago, Florida, Piri√°polis, Punta Gorda, Ciudad Vieja, Barrio Sur, etc. (ciudades de cada departamento)"
        "- operacion: Venta, Alquiler, Alquiler temporal. "
        "- tipo: Apartamento, Campos, Casas, Cocheras, Dep√≥sitos y galpones, Habitaciones, Llave de negocio, Locales, Oficinas, Quintas, Terrenos, Otros inmuebles."
        "- condicion: Nuevo/Usado. "
        "- moneda: USD, UYU. "
        "- precio_min, precio_max, dormitorios_min, dormitorios_max, banos_min, banos_max, cocheras_min, cocheras_max, antiguedad_min, antiguedad_max, superficie_total_min, superficie_total_max, superficie_cubierta_min, superficie_cubierta_max: valores num√©ricos. "
        "- amoblado, terraza, aire_acondicionado, piscina, jardin, ascensor: true/false. "
        "IMPORTANTE: Si el valor original del usuario para ciudad/barrio es m√°s espec√≠fico que el filtro (por ejemplo, 'Pocitos Nuevo' en vez de 'Pocitos'), guarda el valor original como keyword adem√°s del filtro. "
        "Si el usuario indica barrio pero no ciudad, intuir y completar ciudad y departamento correspondiente. Si indica ciudad pero no departamento, intuir departamento correspondiente."
        "Si dice 'a estrenar' se refiere a antig√ºedad 0 a√±os"
        "No inventes filtros nuevos, si el usuario menciona algo para lo que no hay filtro, agr√©galo como keyword. "
        "Ejemplo: 'Apartamento de 2 dormitorios y 2 ba√±os en Pocitos Nuevo, con terraza lavadero, garage para 2 autos, gastos comunes menores a 5.000 pesos y a menos de 3 cuadras de la rambla.' -> filters con tipo='apartamento', dormitorios_min/max=2, banos_min/max=2, ciudad='Pocitos', departamento='Montevideo', terraza=true, cocheras_min/max=2. keywords=['Pocitos Nuevo', 'terraza lavadero', 'garage para 2 autos', 'gastos comunes menores a 5.000 pesos', 'a menos de 3 cuadras de la rambla']. "
        f"\nTexto: {query}\nResponde SOLO JSON sin explicaci√≥n adicional."
    )

    if not API_KEY:
        return {"filters": {}, "remaining_text": query}

    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = await sync_to_async(model.generate_content)(prompt)
        raw = response.text.strip() if hasattr(response, 'text') else str(response)
        # Limpiar bloques ```json ... ``` si existen
        if raw.startswith('```'):
            import re
            m = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', raw, re.DOTALL)
            if m:
                raw = m.group(1)
        try:
            data = json.loads(raw)
        except Exception:
            print(f"[DEPURACI√ìN] No se pudo parsear JSON de IA. Respuesta cruda: {raw[:200]}")
            return {"filters": {}, "remaining_text": query}

        # Normalizar booleans en filters
        for k, v in list(data.get('filters', {}).items()):
            if isinstance(v, str):
                if v.lower() == 'true':
                    data['filters'][k] = True
                elif v.lower() == 'false':
                    data['filters'][k] = False
        data.setdefault('filters', {})
        data.setdefault('remaining_text', '')
        return data
    except Exception as e:
        print(f"[DEPURACI√ìN] Error IA: {e}")
        return {"filters": {}, "remaining_text": query}

def delete_search(request, search_id):
    """Eliminar una b√∫squeda guardada."""
    try:
        from django.shortcuts import redirect
        from django.contrib import messages
        
        success = delete_search_manager(search_id)
        if success:
            messages.success(request, 'B√∫squeda eliminada correctamente')
        else:
            messages.error(request, 'B√∫squeda no encontrada')
            
        return redirect('core:home')
        
    except Exception as e:
        from django.shortcuts import redirect
        from django.contrib import messages
        print(f"[DEPURACI√ìN] Error eliminando b√∫squeda {search_id}: {e}")
        messages.error(request, f'Error al eliminar b√∫squeda: {str(e)}')
        return redirect('core:home')

def search_detail_ajax(request, search_id):
    """Vista AJAX para cargar detalles de b√∫squeda."""
    try:
        search = get_search(search_id)
        if not search:
            return JsonResponse({'error': 'B√∫squeda no encontrada'}, status=404)
        
        results = load_results(search_id)
        advertencias = ["El texto de la b√∫squeda encuentra las palabras por separado en las publicaciones."]
        html = render_to_string('core/search_detail_partial.html', {
            'search': search, 
            'results': results, 
            'advertencias': advertencias
        })
        return JsonResponse({'html': html})
    except Exception as e:
        print(f"[DEPURACI√ìN] Error en search_detail_ajax {search_id}: {e}")
        return JsonResponse({'error': str(e)}, status=500)

def detener_busqueda_view(request):
    """Detener b√∫squeda en progreso."""
    if request.method == 'POST':
        try:
            with search_lock:
                # Marcar todas las b√∫squedas activas para detenerse
                for search_id in list(active_searches.keys()):
                    active_searches[search_id]['stop_requested'] = True
                    print(f"[DEPURACI√ìN] Solicitada parada para b√∫squeda {search_id}")
                
                print(f"[DEPURACI√ìN] B√∫squedas marcadas para detener: {len(active_searches)}")
            
            return JsonResponse({'success': True, 'message': 'Se√±al de parada enviada a b√∫squedas activas'})
        except Exception as e:
            print(f"[DEPURACI√ìN] Error deteniendo b√∫squeda: {e}")
            return JsonResponse({'success': False, 'error': str(e)}, status=500)
    return JsonResponse({'success': False, 'error': 'M√©todo no permitido'}, status=405)

def is_search_stopped(search_id):
    """Verificar si una b√∫squeda debe detenerse."""
    with search_lock:
        return active_searches.get(search_id, {}).get('stop_requested', False)

def register_active_search(search_id):
    """Registrar una b√∫squeda como activa."""
    with search_lock:
        active_searches[search_id] = {'stop_requested': False}
        print(f"[DEPURACI√ìN] B√∫squeda {search_id} registrada como activa")

def unregister_active_search(search_id):
    """Desregistrar una b√∫squeda activa."""
    with search_lock:
        if search_id in active_searches:
            del active_searches[search_id]
            print(f"[DEPURACI√ìN] B√∫squeda {search_id} desregistrada")

@csrf_exempt
@require_POST
def http_search_fallback(request):
    """Fallback HTTP para b√∫squedas cuando WebSockets no funcionan"""
    try:
        data = json.loads(request.body)
        texto = data.get('texto', '')
        filtros_manual = data.get('filtros', {})

        # Extraer informaci√≥n de guardado
        should_save = data.get('guardar', False)
        search_name = data.get('name', '')
        print(f"[HTTP FALLBACK] Iniciando b√∫squeda HTTP: {texto}")
        print(f'üíæ [HTTP FALLBACK] Guardar b√∫squeda: {should_save}, Nombre: "{search_name}"')

        # USAR IA como en el WebSocket consumer
        print('ü§ñ [HTTP FALLBACK] Procesando texto con IA...')
        try:
            ia_result = async_to_sync(analyze_query_with_ia)(texto)
            print(f'ü§ñ [HTTP FALLBACK] Resultado IA: {ia_result}')

            # Fusionar filtros como en el consumer
            filtros_ia = ia_result.get('filters', {})
            filtros_final = filtros_manual.copy()
            for k, v in filtros_ia.items():
                filtros_final[k] = v  # Prioriza IA

            keywords = ia_result.get('keywords', [])
            if isinstance(keywords, str):
                keywords = [keywords] if keywords else []

            print(f'üéöÔ∏è [HTTP FALLBACK] Filtros fusionados: {filtros_final}')
            print(f'üîç [HTTP FALLBACK] Keywords de IA: {keywords}')

            # Guardar b√∫squeda si fue solicitado
            saved_search_id = None
            if should_save:
                print(f'üíæ [HTTP FALLBACK] Iniciando guardado de b√∫squeda: "{search_name}"')
                try:
                    from core.search_manager import create_search
                    from datetime import datetime
                    search_data = {
                        'name': search_name or f'B√∫squeda {datetime.now().strftime("%d/%m/%Y %H:%M")}',
                        'keywords': keywords,
                        'original_text': texto,
                        'filters': filtros_final
                    }
                    created_search = create_search(search_data)
                    saved_search_id = created_search.get('id')
                    print(f'‚úÖ [HTTP FALLBACK] B√∫squeda guardada con ID: {saved_search_id}')
                except Exception as save_error:
                    print(f'‚ùå [HTTP FALLBACK] Error guardando b√∫squeda: {save_error}')
                    # No retornar, continuar con el scraping

        except Exception as e:
            print(f'ü§ñ [HTTP FALLBACK] Error procesando con IA: {e}')
            # Fallback al procesamiento b√°sico
            from .search_manager import procesar_keywords
            keywords = procesar_keywords(texto) if texto else []
            filtros_final = filtros_manual

            # Guardar b√∫squeda si fue solicitado (a√∫n con fallback b√°sico)
            saved_search_id = None
            if should_save:
                print(f'üíæ [HTTP FALLBACK] Iniciando guardado de b√∫squeda (modo fallback): "{search_name}"')
                try:
                    from core.search_manager import create_search
                    from datetime import datetime
                    search_data = {
                        'name': search_name or f'B√∫squeda {datetime.now().strftime("%d/%m/%Y %H:%M")}',
                        'keywords': keywords,
                        'original_text': texto,
                        'filters': filtros_final
                    }
                    created_search = create_search(search_data)
                    saved_search_id = created_search.get('id')
                    print(f'‚úÖ [HTTP FALLBACK] B√∫squeda guardada con ID: {saved_search_id}')
                except Exception as save_error:
                    print(f'‚ùå [HTTP FALLBACK] Error guardando b√∫squeda: {save_error}')
                    # No retornar, continuar con el scraping

        # Ejecutar scraper con los filtros y keywords procesados
        from .scraper import run_scraper
        resultados_scraper = run_scraper(filtros_final, keywords, max_paginas=2, workers_fase1=1, workers_fase2=1) or []
        # Obtener resultados de la base de datos
        from .models import Propiedad
        propiedades = Propiedad.objects.order_by('-id')[:50]  # √öltimas 50 para buscar coincidencias

        resultados = []
        for prop in propiedades:
            if keywords:  # Si hay keywords, filtrar por ellas
                import unicodedata
                def normalizar(texto):
                    return unicodedata.normalize('NFKD', str(texto)).encode('ASCII', 'ignore').decode('ASCII').lower()

                meta = prop.metadata or {}
                caracteristicas_txt = meta.get('caracteristicas', '') or meta.get('caracteristicas_texto', '') or ''
                texto_propiedad = f"{prop.titulo or ''} {prop.descripcion or ''} {caracteristicas_txt}".lower()
                texto_norm = normalizar(texto_propiedad)
                # Keywords puede venir como lista de dicts; extraer 'texto' y 'sinonimos'
                from core.scraper import extraer_variantes_keywords
                keywords_con_variantes = extraer_variantes_keywords(keywords)
                keywords_norm = [normalizar(kw) for kw in keywords_con_variantes]

                # Usar l√≥gica flexible como en el scraper
                from core.scraper import stemming_basico
                texto_stemmed = stemming_basico(texto_norm)
                keywords_stemmed = [stemming_basico(kw) for kw in keywords_norm]

                coincidencias = 0
                for kw_stemmed in keywords_stemmed:
                    if kw_stemmed in texto_stemmed or any(kw_stemmed in word for word in texto_stemmed.split()):
                        coincidencias += 1

                # Si coincide al menos el 70% de las keywords
                if len(keywords_stemmed) > 0 and coincidencias / len(keywords_stemmed) >= 0.7:
                    resultados.append({
                        'title': prop.titulo or 'Sin t√≠tulo',
                        'url': prop.url or '#',
                        'titulo': prop.titulo or 'Sin t√≠tulo',  # Para compatibilidad con search_manager
                        'precio': (f"{meta.get('precio_valor')} {meta.get('precio_moneda','')}".strip() if meta.get('precio_valor') else 'Precio no disponible')
                    })
            else:
                # Si no hay keywords, preferir los enlaces devueltos por el scraper (FASE 1)
                if resultados_scraper:
                    resultados = []
                    for item in resultados_scraper:
                        url = item.get('url')
                        titulo = item.get('title') or item.get('titulo') or 'Publicaci√≥n'
                        if not url:
                            continue
                        resultados.append({
                            'title': titulo,
                            'url': url,
                            'titulo': titulo,
                            'precio': 'Precio no disponible'
                        })
                    break  # Ya poblamos resultados desde el scraper; no seguir iterando propiedades
                else:
                    meta = prop.metadata or {}
                    resultados.append({
                        'title': prop.titulo or 'Sin t√≠tulo',
                        'url': prop.url or '#',
                        'titulo': prop.titulo or 'Sin t√≠tulo',  # Para compatibilidad con search_manager
                        'precio': (f"{meta.get('precio_valor')} {meta.get('precio_moneda','')}".strip() if meta.get('precio_valor') else 'Precio no disponible')
                    })

        # Actualizar b√∫squeda guardada con resultados si existe
        if saved_search_id and resultados:
            print(f'üîÑ [HTTP FALLBACK] Actualizando b√∫squeda {saved_search_id} con {len(resultados)} resultados...')
            try:
                from core.search_manager import update_search
                from datetime import datetime

                # Formatear resultados para el search_manager
                resultados_formatted = [
                    {
                        'titulo': r['titulo'],
                        'url': r['url'],
                        'precio': r['precio']
                    } for r in resultados
                ]

                update_data = {
                    'results': resultados_formatted,
                    'ultima_revision': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                if update_search(saved_search_id, update_data):
                    print(f"update_data: \n\n{update_data}\n\n")
                    print(f'‚úÖ [HTTP FALLBACK] B√∫squeda actualizada con {len(resultados_formatted)} resultados')
                else:
                    print(f'‚ùå [HTTP FALLBACK] No se pudo actualizar la b√∫squeda {saved_search_id}')

            except Exception as update_error:
                print(f'‚ùå [HTTP FALLBACK] Error actualizando b√∫squeda: {update_error}')

        # Exportar CSVs autom√°ticamente tras cada consulta
        try:
            export_all(os.path.join(settings.BASE_DIR, 'exports'))
        except Exception as _e:
            print(f"[DEPURACI√ìN] Exportaci√≥n CSV fall√≥: {_e}")

        return JsonResponse({
            'success': True,
            'matched_publications': resultados[:10],  # Limitar a 10 para no sobrecargar
            'total': len(resultados)
        })

    except Exception as e:
        print(f"[HTTP FALLBACK] Error: {e}")
        return JsonResponse({
            'success': False,
            'error': str(e),
            'matched_publications': []
        })

def csv_export_all(request):
    """Genera CSVs en ./exports/latest y retorna un JSON con la lista de archivos."""
    try:
        base_dir = os.path.join(settings.BASE_DIR, 'exports')
        export_all(base_dir)
        # Prune snapshots antiguos; conservar 0 o 1 seg√∫n preferencia. Dejamos 0 para no acumular.
        prune_old_exports(base_dir, keep=0)
        latest_dir = os.path.join(base_dir, 'latest')
        files = []
        if os.path.exists(latest_dir):
            for name in os.listdir(latest_dir):
                if name.endswith('.csv'):
                    files.append(name)
        # Generar y adjuntar auditor√≠a
        manifest = audit_exports(base_dir)
        return JsonResponse({
            'success': True,
            'dir': latest_dir,
            'files': sorted(files),
            'audit': manifest
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

def csv_export_table(request, table: str):
    """Devuelve CSV para una tabla espec√≠fica on-the-fly (sin tocar disco)."""
    try:
        from django.db import connection
        with connection.cursor() as cur:
            cur.execute(f"SELECT * FROM {table} LIMIT 0")
            headers = [d[0] for d in cur.description]
            cur.execute(f"SELECT * FROM {table}")
            # Build CSV in memory
            import io, csv
            buf = io.StringIO()
            writer = csv.writer(buf)
            writer.writerow(headers)
            while True:
                rows = cur.fetchmany(5000)
                if not rows:
                    break
                for r in rows:
                    writer.writerow(r)
            data = buf.getvalue()
        resp = HttpResponse(data, content_type='text/csv; charset=utf-8')
        resp['Content-Disposition'] = f'inline; filename="{table}.csv"'
        return resp
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def csv_audit_latest(request):
    """Devuelve el manifiesto de auditor√≠a de exports/latest; lo genera si falta."""
    try:
        base_dir = os.path.join(settings.BASE_DIR, 'exports')
        latest_dir = os.path.join(base_dir, 'latest')
        manifest_path = os.path.join(latest_dir, '_manifest.json')
        if not os.path.exists(manifest_path):
            audit = audit_exports(base_dir)
        else:
            with open(manifest_path, 'r', encoding='utf-8') as f:
                audit = json.load(f)
        return JsonResponse({'success': True, 'audit': audit})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def redis_diagnostic(request):
    """Vista de diagn√≥stico para verificar Redis y WebSockets"""
    try:
        from channels.layers import get_channel_layer
        import os
        
        channel_layer = get_channel_layer()
        redis_url = os.environ.get('REDIS_URL', 'No configurada')
        
        diagnostics = {
            'redis_url': redis_url[:50] + '...' if len(redis_url) > 50 else redis_url,
            'channel_layer': str(type(channel_layer)) if channel_layer else None,
            'channel_layer_available': channel_layer is not None,
        }
        
        # Intentar una operaci√≥n simple con channel_layer
        if channel_layer:
            try:
                from asgiref.sync import async_to_sync
                async_to_sync(channel_layer.group_send)("test_group", {
                    "type": "test_message",
                    "message": "test"
                })
                diagnostics['channel_layer_test'] = 'SUCCESS'
            except Exception as e:
                diagnostics['channel_layer_test'] = f'ERROR: {str(e)}'
        
        return JsonResponse(diagnostics)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'redis_url': os.environ.get('REDIS_URL', 'No configurada')[:50] + '...'
        })

def debug_screenshots(request):
    """Vista para mostrar capturas de debug cuando no hay WebSockets disponibles"""
    try:
        debug_file = os.path.join('static', 'debug_screenshots', 'latest_screenshots.json')
        screenshots = []
        
        if os.path.exists(debug_file):
            try:
                with open(debug_file, 'r', encoding='utf-8') as f:
                    screenshots = json.load(f)
            except:
                screenshots = []
        
        # Ordenar por timestamp m√°s reciente primero
        screenshots.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        context = {
            'screenshots': screenshots,
            'has_screenshots': len(screenshots) > 0
        }
        
        return render(request, 'core/debug_screenshots.html', context)
        
    except Exception as e:
        return JsonResponse({'error': str(e)})


@require_POST
def nueva(request):
    """Crea una nueva b√∫squeda desde el formulario simple y redirige a home (302)."""
    try:
        data = request.POST
        filtros = {
            'departamento': data.get('departamento', '').strip(),
            'ciudad': data.get('ciudad', '').strip(),
            'operacion': data.get('operacion', '').strip(),
            'tipo': data.get('tipo', '').strip(),
        }
        # keywords puede venir como 'a,b,c'
        raw_keywords = data.get('keywords', '') or ''
        keywords = [k.strip() for k in raw_keywords.split(',') if k.strip()]

        search_data = {
            'name': data.get('name', 'B√∫squeda r√°pida'),
            'original_text': data.get('original_text', ''),
            'filters': filtros,
            'keywords': keywords,
        }
        from .search_manager import create_search
        create_search(search_data)
        return redirect('core:home')
    except Exception as e:
        # Mantener compatibilidad del test: ante error devolver 302 a home para no romper flujo
        try:
            return redirect('core:home')
        except Exception:
            return JsonResponse({'error': str(e)}, status=500)
